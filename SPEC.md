# Alfred Model Bench â€” Specification

## Overview
Custom AI model benchmark tailored to OpenClaw/Alfred workflows. Tests models via OpenRouter (cloud) and Ollama (local) against real-world scenarios, scores them, and produces a web dashboard with results.

## Architecture

```
alfred-model-bench/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.yaml              # Models to test, API keys, settings
â”œâ”€â”€ scenarios/               # Test scenario definitions (YAML)
â”‚   â”œâ”€â”€ tool_orchestration.yaml
â”‚   â”œâ”€â”€ instruction_following.yaml
â”‚   â”œâ”€â”€ email_triage.yaml
â”‚   â”œâ”€â”€ judgment_calls.yaml
â”‚   â”œâ”€â”€ structured_output.yaml
â”‚   â””â”€â”€ long_context.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ runner.py            # Main benchmark runner
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ openrouter.py    # OpenRouter API client
â”‚   â”‚   â””â”€â”€ ollama.py        # Ollama local client
â”‚   â”œâ”€â”€ evaluator.py         # Scoring engine (automated checks)
â”‚   â”œâ”€â”€ external.py          # External benchmark data fetcher
â”‚   â””â”€â”€ report.py            # Generate results JSON for dashboard
â”œâ”€â”€ results/                 # Benchmark run outputs (JSON)
â”œâ”€â”€ dashboard/               # Static web dashboard
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ style.css
â””â”€â”€ run.py                   # CLI entry point
```

## Test Scenarios (6 categories)

### 1. Tool Orchestration
- Prompt: "Check my calendar for today and create a Things task for each meeting with prep notes"
- Tests: correct multi-step tool call generation, parameter accuracy, sequencing
- Scoring: JSON schema validation of tool calls, correct ordering, no hallucinated tools

### 2. Instruction Following
- Prompt: system prompt (simplified AGENTS.md) + "You receive a heartbeat. What do you do?"
- Tests: compliance with operational docs, correct procedure
- Scoring: checklist of expected behaviors (read HEARTBEAT.md, check items, respond correctly)

### 3. Email Triage
- Prompt: 10 sample emails with varying urgency â†’ "Classify each by alert level (ðŸ”´ðŸŸ¡ðŸŸ¢) and draft responses for ðŸ”´"
- Tests: classification accuracy, response quality
- Scoring: accuracy vs ground truth labels, response relevance

### 4. Judgment Calls (when to speak vs stay silent)
- Prompt: 5 group chat scenarios â†’ "Should you respond? If yes, what?"
- Tests: social awareness, appropriate silence
- Scoring: binary correct on speak/silent + quality of response when speaking

### 5. Structured Output
- Prompt: "Create a Notion page with these fields: [spec]. Return the API call JSON."
- Tests: valid JSON, correct Notion API structure, all fields populated
- Scoring: JSON validity, schema compliance, field completeness

### 6. Long Context
- Prompt: 20K+ token context (memory files + conversation) + question about specific detail
- Tests: retrieval accuracy from long context, no hallucination
- Scoring: exact match or semantic similarity to ground truth

## Scoring

Each scenario produces a score 0-100:
- **Automated checks** (tool call validity, JSON schema, exact match) = binary pass/fail per criterion
- **LLM-as-judge** for subjective quality (using Claude as evaluator) = 0-100 rubric score
- Final score = weighted average

## Models to Test (initial set)

Cloud (via OpenRouter):
- claude-sonnet-4-20250514
- claude-opus-4-20250514
- gpt-4o
- gpt-4o-mini
- gpt-o3
- gpt-o4-mini
- gemini-2.5-pro
- gemini-2.5-flash
- deepseek-chat-v3
- deepseek-r1
- llama-4-maverick
- qwen-2.5-72b

Local (via Ollama, optional):
- llama3.2:8b
- qwen2.5:14b
- mistral:7b

## Dashboard
Static HTML/JS/CSS. Reads results JSON. Shows:
- Model comparison table (sortable by category)
- Radar chart per model (category scores)
- Overall ranking
- Cost per run (from OpenRouter pricing)
- Recommendation matrix: model Ã— use case

## CLI Usage

```bash
# Run full benchmark
python run.py --all

# Run specific scenario
python run.py --scenario tool_orchestration

# Run specific model
python run.py --model claude-sonnet-4-20250514

# Generate dashboard
python run.py --report
```

## Config (config.yaml)

```yaml
openrouter_api_key_file: ~/.config/openrouter/api_key
ollama_url: http://localhost:11434
evaluator_model: claude-sonnet-4-20250514  # for LLM-as-judge
models:
  - id: anthropic/claude-sonnet-4-20250514
    provider: openrouter
  # ... etc
```
